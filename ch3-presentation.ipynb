{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721f3435",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Chapter 3: Machine Learning with Shallow Neural Networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febaf717",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Goal**: Explore optimization methods in machine learning (ML) using simple neural network (NN) architectures.\n",
    "- **Focus**: \n",
    "  - How increased data availability impacts accuracy.\n",
    "  - Trade-offs between simple ML models vs. complex NNs based on data availability.\n",
    "  - How NNs can make better predictions with large datasets.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b476d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " **The effect of increased data availability on accuracy**.\n",
    "\n",
    "![Perceptron Diagram](image1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9370b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Simple ML models**: Easier to train with smaller datasets.\n",
    "- **Complex Neural Networks**: More effective when there’s a lot of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9aa8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks as Networks of Simple Units\n",
    "- Deep learning models consist of **connected simple computational units**.\n",
    "- Examples:\n",
    "  - **Perceptron** for linear classification.\n",
    "  - **Linear and logistic regression**.\n",
    "- Stacking these basic units leads to advanced models (e.g., **CNNs (convolutional) for images** or **RNNs (recurrent) for sequential data**)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8729b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- **Supervised models**: Correspond to linear models like:\n",
    "  - **Least-squares regression**\n",
    "  - **Support Vector Machines (SVMs)**\n",
    "  - **Logistic regression**\n",
    "- **Unsupervised models**: Handle tasks like:\n",
    "  - **Dimensionality reduction**\n",
    "  - **Matrix factorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee8cc5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least Squares Regression and Neural Architectures\n",
    "- Neural architectures for regression tasks are based on the **perceptron**.\n",
    "- **Main differences**: \n",
    "  - Activation function in the final layer.\n",
    "  - Loss function applied to outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170ba58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Structure\n",
    "- Single-layer network:\n",
    "  - **Input nodes**: \\(d\\) inputs.\n",
    "  - **Output node**: Single binary output.\n",
    "- **Weight vector** :$$(W = [w1,...,w_d]^T$$\n",
    "  - Learned during training.\n",
    "  - Assumed to be a **column vector**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335f9c1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Slide 7: Revisiting the Perceptron\n",
    "- **Training instance**: Rows of data matrices --> row vector\n",
    "- **Sign function** applied to linear combination of weights and inputs to get binary classification.\n",
    "![Activation function](signfunc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5917805b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent and Perceptron Updates\n",
    "- **Goal**: Minimize misclassifications and ensure prediction is close to the observed value. \n",
    "- **Update rule**:\n",
    "  $$\n",
    "  W \\leftarrow W + \\alpha(y_i - \\hat{y}_i)X_i^T\n",
    "  $$\n",
    "  where:\n",
    "  - $\\alpha\\ \\text{= learning rate}$\n",
    "  - $y_i \\text{ = true label}$\n",
    "  - $\\hat{y}_i\\ \\text{ = predicted label}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3872083a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Loss Function\n",
    "- **Perceptron criterion** (loss function):\n",
    "$$\n",
    "  L_i = \\max(0, -y_i(W \\cdot X_i^T)\n",
    "  $$\n",
    "  - Updates focus on misclassified points only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5851d1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "- **SGD**: Updates weights after *each* training example.\n",
    "- **Benefits**:\n",
    "  - Faster training.\n",
    "  - Introduces useful randomness.\n",
    "  \n",
    "  \n",
    "  ![Activation function](SGD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16c343e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Summary\n",
    "- **Effective** for linear classification problems.\n",
    "- **Challenges**: Struggles with non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e20318b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Least Squares Regression\n",
    "- **Goal**: Predicting **real values** (not classification).\n",
    "  ![Activation function](LSR.png)\n",
    "- **Loss Function**:\n",
    "  - The portion of the loss for the \\(i\\)-th training instance is defined as the **squared error**.\n",
    "    ![Activation function](LSRloss.png)\n",
    "- **Architecture**: Similar to perceptron but with **squared loss** instead of perceptron criterion.\n",
    "  - **Key Difference**: Predictions are real values (not binary classifications).\n",
    "  - **Gradient**: Calculated as the derivative of the loss function.\n",
    "  - **Step-size**: Learning rate $\\alpha\\$.\n",
    "  ![Activation function](LSRSGD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f4babd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Widrow-Hoff Learning\n",
    "- **Also known as**: Least Mean Squares (LMS) Algorithm.\n",
    "- **Key Application**: Applies **least squares regression** to binary classification.\n",
    "- **Differences from Perceptron**:\n",
    "  - Predictions are real values without applying a sign function.\n",
    "  - **More precise updates** as errors can take any real value.\n",
    "- Loss function is just the squared error \n",
    "- **Weight Update Rule**:\n",
    "  $$ W \\leftarrow W + \\alpha(y_i - \\hat{y}_i)X_i^T $$\n",
    " - For binary responses:\n",
    "  $$ W \\leftarrow W + \\alpha y_i(1 - y_i\\hat{y}_i)X_i^T$$\n",
    "- **Key Weakness**: Treats binary labels as real-valued targets, which can lead to suboptimal solutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca635b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Closed-Form Solutions (without Gradient Descent)\n",
    "- **When applicable**: In some cases, the weight vector $W$ can be computed analytically.\n",
    "- **Uses the pseudo-inverse** of the feature matrix $D$:\n",
    "  $$\n",
    "  W = (D^T D)^{-1} D^T y\n",
    "  \\$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae81dc7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines (SVMs) \n",
    "- **Purpose**: Binary classification model, predicting whether data points belong to one of two classes.\n",
    "- **Architecture**: Similar to Widrow-Hoff but differs in **loss function**.\n",
    "- **Prediction**: \n",
    "\n",
    "  $$ \\hat{y} = W \\cdot X_i^T \\$$\n",
    "  - **Weight vector \\(W\\)** learned during training.\n",
    "  - **Sign function** applied at test time to assign class labels (-1, +1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3157e6f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVM: Hinge Loss Function\n",
    "---\n",
    "### Hinge Loss in SVMs\n",
    "- Loss function: **$L = max(0, 1 - y_i \\hat{y})$**\n",
    "- **No penalty** if $y_i \\hat{y}  > 1$ (correct predictions)\n",
    "- Penalizes **incorrect** or **low-confidence** predictions where $y_i \\hat{y} < 1 $\n",
    "- Hinge loss avoids over-penalization\n",
    "\n",
    "  ![hingeloss](hinge.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f353493a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Concept of Margin\n",
    "- Predictions must be confidently correct\n",
    "    - Positive class: prediction **ŷ > 1** to avoid penalties\n",
    "    - Negative class: prediction **ŷ < -1**\n",
    "- Ensures stability on noisy data by ignoring overconfident errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b232d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD) in SVMs\n",
    "---\n",
    "### SVMs vs Perceptron\n",
    "- Both use **SGD**\n",
    "    - **Perceptron** updates for misclassified points\n",
    "    - **SVM** updates for both misclassified and low-confidence correct points (where **yᵢŷ < 1**)\n",
    "- Gradient descent ensures weights only updated when **yᵢŷ < 1**, aiming for confidently correct predictions\n",
    "\n",
    "  ![hingeloss](SVMSGD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70d3fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key Advantages\n",
    "- **Stability on noisy data:** Margin improves stability even if data isn’t perfectly linearly separable\n",
    "- **Convergence:** More likely to converge than Perceptron\n",
    "- **Optimal stability:** Known as the “perceptron of optimal stability” because of robustness\n",
    "- Updates occur more frequently, leading to more stability and reliability than perceptrons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c39760e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Logistic Regression\n",
    "---\n",
    "### Probabilistic Binary Classification Model\n",
    "- Predicts probability of an instance belonging to one of two classes\n",
    "- Uses the **sigmoid function** applied to weighted input features\n",
    "- Converts the weighted sum of features into probability\n",
    "  ![hingeloss](sigmoid.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb3e5fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression Loss Function\n",
    "---\n",
    "### Maximizing Likelihood\n",
    "- **Goal:** Maximize likelihood of correctly predicting class labels\n",
    "    - For positive samples (**y = 1**): Maximize **ŷ**\n",
    "    - For negative samples (**y = -1**): Maximize **1 - ŷ**\n",
    "  - product of all probabilities of training samples gives the likelihood function:\n",
    "  ![hingeloss](loglikelihood.png)\n",
    "  - maximizing product is challenging so log-likelihood is used instead: \n",
    "    ![hingeloss](LL.png)\n",
    "- Loss function (log-likelihood) simplified: \n",
    "$$L = log(1 + exp(-y_i\\cdot W \\cdot X_i^T))$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce763a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression Gradient Descent\n",
    "---\n",
    "### Gradient Descent Update\n",
    "- Weights updated based on the probability of misclassification\n",
    "- Higher probability of error leads to larger adjustments to weights\n",
    "- More stable and robust than perceptron\n",
    "\n",
    " ![hingeloss](LRGD.png)\n",
    "  ![hingeloss](LRSGD.png)\n",
    "  \n",
    " - learning rate controls size of updates\n",
    " - denominator accounts for probability of error for each instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45b832",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Perceptron vs SVM\n",
    "- Similar structure, but SVM hinge loss is shifted one unit to the right\n",
    "- SVM only updates when prediction is not confidently correct, adding stability on noisy data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9df62c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Key Takeaways\n",
    "- All models use **stochastic gradient descent** for optimization\n",
    "- **Perceptron** updates for misclassified points\n",
    "- **SVM** updates for both incorrect and low-confidence points\n",
    "- **Logistic Regression** adjusts based on the probability of error\n",
    "  ![hingeloss](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03d3d5",
   "metadata": {},
   "source": [
    "   ![hingeloss](table.png)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576b06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Architectures for Multiclass Models\n",
    "---\n",
    "### Overview\n",
    "- Neural architectures extend binary classification models to handle multiple classes (k classes).\n",
    "- Three key models:\n",
    "  - Multiclass Perceptron\n",
    "  - Weston-Watkins SVM\n",
    "  - Multinomial Logistic Regression (Softmax Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339195ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiclass Perceptron\n",
    "---\n",
    "### Extending Perceptron to Multiclass\n",
    "- **Goal:** Assign input to the class with the highest score based on a set of learned weights\n",
    "- Training data: $(X_i, c(i))$ where $X_i$ is the feature vector and $c(i)$ is the correct class for the ith instance\n",
    "- Weights: Separate weight vectors for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49deed3e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction Function for Multiclass Perceptron\n",
    "---\n",
    "### Prediction Function:\n",
    "$$ \\hat{C}(i) = argmax(W_r \\cdot X_i^T)$$\n",
    "  - **Ĉ(i):** Predicted class (class with the highest dot product)\n",
    "  - The model assigns the class **r** with the highest score to the predicted class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5793cb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient:\n",
    "- **Negative gradient:** Correct class predicted, but it doesn't have the highest dot product\n",
    "- **Positive gradient:** Wrong class has higher score than the correct class\n",
    "- **Gradient is 0 (correct class):** Correct class has the highest score (no update)\n",
    "   ![hingeloss](multiclassgradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710bad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weight Updates:\n",
    "\n",
    "   ![hingeloss](multiclassSGD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fc4c8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Weston-Watkins SVM\n",
    "---\n",
    "### Extension of Multiclass Perceptron\n",
    "- Learns k weight vectors, one for each class\n",
    "- **Goal:** Ensure the correct class’s score is sufficiently higher than other classes by maintaining a margin\n",
    "- Prediction follows the same formula as the multiclass perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02953f3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weston-Watkins SVM Loss Function\n",
    "---\n",
    "### Loss Function:\n",
    "- Penalizes incorrect predictions based on how close the incorrect classes are to the correct class\n",
    "- **Goal:** Ensure the correct class $c(i)$ scores higher than all other classes by at least 1 unit (margin)\n",
    "\n",
    "\n",
    "   ![hingeloss](WWLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06238b8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Weston-Watkins SVM Gradient and Updates\n",
    "---\n",
    "### Gradient:\n",
    "- Uses a 0/1 indicator function: 1 if the jth class score is too close or higher than the correct class’s score\n",
    "- **Loss = 0** when the correct class has a high score with sufficient margin\n",
    "- Regularization may be used to shrink weight vectors and prevent overfitting\n",
    "\n",
    "   ![hingeloss](WWGD.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104653d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key takeaways:\n",
    "- **Multiple updates**: updates weights for all incorrect classes that score too closely to the correct class\n",
    "- **Margin concept**: correct class must not only score the highest, but also maintain margin of at least 1 unit from other class scores\n",
    "- **EVEN IF CORRECT CLASS PREDICTED**: updates can occur if other classes are too close in score, ensuring better decision boundaries\n",
    "- Provides more robust decision boundaries by penalizing borderline predictions, ensuring better separation between classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f193ca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multinomial Logistic Regression (Softmax Classifier)\n",
    "---\n",
    "### Probabilistic Model for Multiclass Classification\n",
    "- Extends logistic regression to multiclass\n",
    "- **Goal:** Assign each input instance to one of k classes using a probabilistic approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcb091",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax Prediction Function\n",
    "---\n",
    "### Prediction/Activation:\n",
    "- Produces a probability distribution over all possible classes for each input instance\n",
    "- **Prediction for class r:** Probability that input belongs to class r\n",
    "- Higher dot product correspond to higher probabilities\n",
    "- Probabilities across all classes sum to 1 (normalized probabilities)\n",
    "   ![hingeloss](multinomialpredictionfunc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06b47e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss Function for Multinomial Logistic Regression\n",
    "---\n",
    "### Cross-Entropy Loss:\n",
    "- Measures how well predicted probabilities align with actual class labels\n",
    "- **Goal:** Maximize the probability of the correct class by minimizing the cross-entropy loss\n",
    "\n",
    "- If the model assigns a low probability to the correct class, the loss is high\n",
    "- Low loss occurs when the correct class has a high probability\n",
    "\n",
    "   ![hingeloss](CEL.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c7ab32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Gradient and Weight Updates for Softmax Classifier\n",
    "---\n",
    "### Weight Updates:\n",
    "- For correct class $r = c(i):$ Gradient decreases weight when the probability of the correct class is low\n",
    "- For other classes $r \\neq c(i):$ Gradient increases the weights of incorrect classes based on their predicted probabilities\n",
    "- **SGD** is used for weight updates\n",
    "   ![hingeloss](softmaxgradient.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30adeb7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Key Differences:\n",
    "- **Probabilistic approach:** Assigns probabilities rather than strict classifications\n",
    "- All weights are updated for each instance\n",
    "- Even when the correct class is predicted, weight updates are made to fine-tune probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac390a12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advantages:\n",
    "- A robust choice for tasks where finely-tuned probabilities are important\n",
    "- Updates occur until the loss becomes negligible or a fixed number of epochs is reached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0622a07",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### Summary of Multiclass Models:\n",
    "- **Multiclass Perceptron:** Extends perceptron, updates only for wrong predictions\n",
    "- **Weston-Watkins SVM:** Adds margin, penalizes borderline predictions, uses regularization\n",
    "- **Softmax Classifier:** Probabilistic approach, cross-entropy loss, updates all weights for each instance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e842e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning with Autoencoders\n",
    "---\n",
    "### Autoencoders Overview:\n",
    "- A type of neural network used for unsupervised learning\n",
    "- **Purpose:** Learn efficient representations of input data by compressing it into a lower-dimensional space and reconstructing it as closely as possible to the original input\n",
    "- Applications:\n",
    "  - Dimensionality reduction\n",
    "  - Feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c99b30f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction in Autoencoders\n",
    "---\n",
    "### Linear vs Deep Autoencoders:\n",
    "- **Simple linear autoencoders:** Resemble traditional dimensionality reduction techniques like singular value decomposition (SVD)\n",
    "- **Deep autoencoders:** Introduce nonlinear layers for more complex embeddings\n",
    "- Flexibility with deep architectures allows for greater optimization power through backpropagation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a1131c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoder Architecture\n",
    "---\n",
    "### Architecture:\n",
    "- **Input and output layers:** Same number of units to ensure the model can reconstruct input from the compressed form\n",
    "- **Hidden layers:** Constricted layers (bottlenecks) reduce dimensionality by forcing the network to learn compressed representations\n",
    "\n",
    "\n",
    "   ![hingeloss](AE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f855e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder and Decoder\n",
    "---\n",
    "### How it Works:\n",
    "- **Encoder:** Compresses input into a smaller representation (code)\n",
    "- **Code:** The most constricted layer, containing the reduced representation\n",
    "- **Decoder:** Reconstructs original input from the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a434ae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Reconstruction and Loss Function\n",
    "---\n",
    "### Reconstruction:\n",
    "- The output attempts to reconstruct the original input\n",
    "- With fewer units in the middle layer, the network is forced to learn a lossy compressed representation\n",
    "\n",
    "### Loss Function:\n",
    "- Measures sum of squared differences between input and reconstructed output\n",
    "- Adjusts weights to minimize the reconstruction error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d55b12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Autoencoders\n",
    "---\n",
    "### Linear Autoencoder with a Single Hidden Layer:\n",
    "- Neural network-based matrix factorization\n",
    "- Used for tasks like dimensionality reduction and recommender systems\n",
    "- Objective: Decompose input matrix **D** into two smaller matrices: **D = UVᵀ**\n",
    "    - minimizing sum of squared differences between the original matrix and its reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81616057",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoding/Decoding Process in Matrix Factorization\n",
    "---\n",
    "### Encoding:\n",
    "- Maps input vector to the reduced representation using encoder weights\n",
    "- $u = X_iW^T$ (where $W$ is the weight matrix for compression)\n",
    "\n",
    "### Decoding:\n",
    "- Reconstructs input using the hidden layer activation and decoder weights\n",
    "- $X = uV^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c72d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders and Singular Value Decomposition (SVD)\n",
    "---\n",
    "### Connections with SVD:\n",
    "- A single-layer linear autoencoder solves a matrix factorization problem similar to SVD by minimizing the Frobenius norm of the reconstruction error\n",
    "- Key difference: SVD has a unique orthonormal solution, while autoencoders may find multiple equivalent solutions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69a6ede",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weight Sharing in Autoencoders\n",
    "---\n",
    "### Weight Sharing:\n",
    "- Autoencoders often share weights between the encoder and decoder for symmetry\n",
    "- $W = V^T$\n",
    "- same weight matrix V is used to compress the input data and to reconstruct it from the reduced representation\n",
    "- Reduces parameters, improves generalization to useen data, and simplifies model training without significant loss in reconstruction accuracy\n",
    "- in backpropogation: half the number of gradient values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786b1f4d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonlinear Activation Functions and Depth\n",
    "---\n",
    "### Nonlinear Activation and Depth:\n",
    "- Nonlinear functions like ReLU or sigmoid allow autoencoders to handle more complex patterns than SVD\n",
    "- **Deep Autoencoders:** Learn hierarchical representations of data through multiple hidden layers, enabling more powerful dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8885ae1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Representation power of deep autoencoders\n",
    "- Beneficial for images or complex structures that require nonlinear dimensionality reduction\n",
    "- Example: Deep autoencoder can flatten spiral into 2d representation while preserving structure (unlike SVD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21c320",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Deep Autoencoders\n",
    "---\n",
    "### Practical Applications:\n",
    "- Dimensionality reduction\n",
    "- Feature learning\n",
    "- Data denoising\n",
    "- Recommender systems\n",
    "- Flexibility: Deep autoencoders can capture complex patterns not possible with linear models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098137a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Slide 12: Autoencoders for Visualization\n",
    "---\n",
    "### Visualization:\n",
    "- Autoencoders can map complex, high-dimensional data into 2D spaces, revealing well-separated clusters\n",
    "- 2D embeddings allow visualization of the class structure in data\n",
    "\n",
    "   ![hingeloss](AEgraphs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4836a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Note on class structure:\n",
    "- Class structure in data refers to the organization of data points according to their class labels. \n",
    "- A well-defined class structure means that points from different classes are separated, while points from the same class form clusters. \n",
    "- clear class structure helps improve classification accuracy and can be visualized using dimensionality reduction techniques like autoencoders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9a0703",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders vs t-SNE(t-distributed Stochastic Neighbor Embedding)\n",
    "---\n",
    "### Autoencoders vs t-SNE:\n",
    "- Autoencoders generalize better to new data, while t-SNE requires recomputing the embedding for new points\n",
    "- Autoencoders provide a flexible and scalable solution for ongoing visualization tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a065db2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application to Outlier Detection\n",
    "---\n",
    "### Outlier Detection:\n",
    "- Autoencoders detect outliers by identifying instances that are difficult to reconstruct\n",
    "- **Outlier scores** are computed from the differences between the original and reconstructed matrix entries\n",
    "    - Summing squared differences for each **row** helps find outlier data points\n",
    "    - Summing squared differences for each **column** helps identify outlier features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d33af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application to Multimodal Embeddings\n",
    "---\n",
    "### Multimodal Embeddings:\n",
    "- Autoencoders can embed multimodal data (e.g., text and images) into a joint latent space\n",
    "- This unified representation simplifies handling heterogeneous data in machine learning tasks\n",
    "\n",
    "   ![hingeloss](multimodal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00956604",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benefits of Autoencoders\n",
    "---\n",
    "### Benefits:\n",
    "- Good at handling new data: can compute reductions through trained networks\n",
    "- Flexible network design: easy to adjust layers, activation functions, and architecture\n",
    "- **Modularity:** Simplifies experimentation with various architectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e3447",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoencoders: Key Takeaways\n",
    "---\n",
    "### Key Points:\n",
    "- Autoencoders are powerful for unsupervised learning tasks like dimensionality reduction, feature learning, and visualization\n",
    "- Deep autoencoders with nonlinear activation capture more complex data patterns\n",
    "- Weight sharing improves model efficiency, and autoencoders outperform other methods like t-SNE in terms of flexibility and generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c00220",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommender Systems with Neural Architectures\n",
    "---\n",
    "### Overview:\n",
    "- Recommender systems work with a ratings matrix $D$, where each entry $(i, j)$ represents the rating given by user $i$ for item $j$\n",
    "- Many entries in $D$ are missing, making traditional autoencoder (AE) architectures difficult to apply\n",
    "- **Goal:** Learn k-dimensional parameter vectors $u_i$ (for users) and $v_j$ (for items) to approximate the rating matrix $D$ using a neural network approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c0816f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenges with Missing Data\n",
    "---\n",
    "### Sparse Ratings Matrix:\n",
    "- The ratings matrix $D$ is sparse, meaning most ratings are missing\n",
    "- **Solution:** Use a triplet-centric output: $(RowId, ColumnId, Rating)$ for learning, rather than a fully specified matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800a4a75",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network for Recommender Systems\n",
    "---\n",
    "### Architecture:\n",
    "- A **one-hot encoded row index** is fed into the input layer, where each input represents a user\n",
    "- The network contains **k hidden units** (corresponding to the rank of factorization) and **d output units** (one for each item)\n",
    "- Output layer predicts all ratings for a user, even though only a small subset of ratings is typically observed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accad8be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Neural Recommender Systems\n",
    "---\n",
    "### Training Approaches:\n",
    "1. **Row-wise training:** One-hot encoded row index is input, loss computed for observed ratings\n",
    "2. **Element-wise training:** A single triplet $(i, j, rating)$ updates weights for the user $i$ and item $j$ using observed rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816869f2",
   "metadata": {},
   "source": [
    "\n",
    "### Row-wise Training:\n",
    "- **Input:** One-hot encoded index of a user, representing a row in the ratings matrix\n",
    "- Loss is computed for the observed ratings, ignoring missing values\n",
    "- Backpropagation updates weights based on available ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667041e",
   "metadata": {},
   "source": [
    "\n",
    "### Element-wise Training:\n",
    "- **Input:** A single triplet $(i, j, rating)$\n",
    "- Loss is computed for the specific rating in the triplet, and backpropagation updates the weights based on this rating\n",
    "- Allows for more granular training compared to row-wise training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9618ef92",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Matrix Factorization through Neural Networks\n",
    "---\n",
    "### Matrix Factorization with Neural Networks:\n",
    "- When a one-hot encoded row is input, the network predicts the user’s ratings for all items by pulling the corresponding row of the $UV$ matrix\n",
    "- Backpropagation updates weights only where observed ratings are available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37d2bf7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Benefits of Neural Architectures in Recommender Systems\n",
    "---\n",
    "### Benefits:\n",
    "- **Flexibility:** Supports multiple hidden layers to create more powerful models\n",
    "- **Customizability:** Can handle different types of data\n",
    "  - Logistic layers for binary data\n",
    "  - Softmax layers for categorical data\n",
    "- **Modularity:** Easy to experiment with models using backpropagation for optimization\n",
    "\n",
    "\n",
    "   ![hingeloss](RS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de6949",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Embedding with Word2Vec\n",
    "---\n",
    "### Overview:\n",
    "- **Word2Vec**: Neural network-based method for learning word embeddings from large text data\n",
    "- **Word Embeddings**: Represent words in a continuous vector space where semantically similar words are close to each other\n",
    "- Learns embeddings by analyzing word contexts within sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a846cd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context in Word Embeddings\n",
    "---\n",
    "### Word-Word Context Matrix:\n",
    "- **(i, j)** entry in matrix represents how often word **j** appears in the context of word **i**\n",
    "- Context: Surrounding words within a window of size 2t (excluding the central target word) where t is the number of words on either side of the target word\n",
    "\n",
    "### Methods for Generating Embeddings:\n",
    "1. **Matrix Factorization**: Directly factorizing the word-word context matrix\n",
    "2. **Neural Model**: Training a neural network to predict relationships between words and contexts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddec5269",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec Models\n",
    "---\n",
    "### Continuous Bag-of-Words (CBOW):\n",
    "- Predicts the target word from its surrounding context words\n",
    "- Averages context words to predict the target word\n",
    "\n",
    "### Skip-Gram Model:\n",
    "- Predicts the surrounding context words from a given target word\n",
    "- Can use multinomial or Bernoulli models (with negative sampling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe92620",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Embedding with Continuous Bag-of-Words (CBOW)\n",
    "---\n",
    "### CBOW Model Structure:\n",
    "1. **Input Layer**: $m$ one-hot encoded vectors (one for each context word)\n",
    "2. **Hidden Layer**: $p$ units (dimensionality of word embeddings)\n",
    "    - The hidden layer output is computed by averaging the embeddings of all the context words.\n",
    "3. **Output Layer**: $d$ nodes (one for each word) to predict the target word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51532b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output Layer and Softmax Function\n",
    "---\n",
    "### Output Layer:\n",
    "- Multiplies the hidden layer’s output by the weight matrix **V** to produce a vector representing unnormalized probabilities\n",
    "\n",
    "### Softmax Function:\n",
    "- Converts unnormalized scores into probabilities for predicting the target word\n",
    "\n",
    "   ![hingeloss](wordvec.png)\n",
    "   ![hingeloss](wordvecdefs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eff354",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CBOW Training and Loss Function\n",
    "---\n",
    "### Training and Loss:\n",
    "- Trained by minimizing negative log-likelihood\n",
    "- **Loss Function**: Measures how far the model’s prediction is from the true target word\n",
    "- $L =−log(\\hat{y}_r)$\n",
    "\n",
    "### Backpropagation:\n",
    "- Updates weights in matrices **U** (context words) and **V** (target words) based on prediction error\n",
    "   ![hingeloss](update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf9d8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Skip-Gram Model in Word2Vec\n",
    "---\n",
    "### Skip-Gram Model:\n",
    "- Predicts context words given a target word\n",
    "- **Goal**: Maximize the likelihood of observing context words for a given target word\n",
    "\n",
    "### Model Structure:\n",
    "1. **Input Layer**: One-hot encoded target word\n",
    "2. **Hidden Layer**: p units for dimensionality of word embeddings. The rows represent embeddings for each word\n",
    "   ![hingeloss](hidden.png)\n",
    "3. **Output Layer**: Predicts a multinomial distribution over possible context words\n",
    "   ![hingeloss](output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd24f5a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skip-Gram Loss Function and Training\n",
    "---\n",
    "### Loss Function:\n",
    "- Based on negative log-likelihood, measuring how well the model predicts context words\n",
    "   ![hingeloss](SGloss.png)\n",
    "\n",
    "\n",
    "### Backpropagation:\n",
    "- Error calculated between predicted and actual context words, updating embeddings in **U** and **V**\n",
    "\n",
    "   ![hingeloss](backprops.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d400c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison of CBOW and Skip-Gram Models\n",
    "---\n",
    "### Key Differences:\n",
    "- **CBOW**: Predicts a single target word from multiple context words, faster to train\n",
    "- **Skip-Gram**: Predicts multiple context words from a single target word, better for rare words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d04fb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Skip-Gram with Negative Sampling (SGNS)\n",
    "---\n",
    "### Skip-Gram with Negative Sampling (SGNS):\n",
    "- Focuses on predicting whether a word-context pair is real (positive) or random (negative)\n",
    "- Uses **sigmoid activation** for binary predictions\n",
    "- selects k negative samples for each positive sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a590f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SGNS Training and Objective Function\n",
    "---\n",
    "### Positive and Negative Samples:\n",
    "- **Positive Samples**: Word-context pairs from the context window\n",
    "- **Negative Samples**: Randomly selected word-context pairs outside the window\n",
    "   - aim to predict that these are false\n",
    "\n",
    "### Objective Function:\n",
    "- Summing probabilities for positive and negative samples using a logistic loss function\n",
    "\n",
    "   ![hingeloss](objective.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217fa9dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SGNS as Logistic Matrix Factorization:\n",
    "- Similar to recommender systems where triplets (WordID, ContextWordID, 0/1) predict values\n",
    "- SGNS predicts word-context pairs similarly to how recommender systems predict user-item interactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4ce63",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective Function and Gradient Descent\n",
    "---\n",
    "### Objective Function:\n",
    "- Minimizes error between observed word-context pairs and predicted values\n",
    "   ![hingeloss](log.png)\n",
    "\n",
    "### Gradient Descent:\n",
    "- Both SGNS and logistic matrix factorization use stochastic gradient descent (SGD) to optimize word embeddings\n",
    "   ![hingeloss](gradient.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b20d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec vs Matrix Factorization\n",
    "---\n",
    "### Key Differences:\n",
    "- **SGNS**: Uses sigmoid for binary predictions, negative sampling to reduce computational costs\n",
    "- **Matrix Factorization**: Uses linear layers, but logistic matrix factorization closely resembles SGNS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc716b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical Benefits of SGNS\n",
    "---\n",
    "### Efficiency and Accuracy:\n",
    "- SGNS improves efficiency by focusing on negative samples\n",
    "- Helps the model distinguish between true and false word-context pairs, improving prediction accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4100ba25",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple Neural Architectures for Graph Embeddings\n",
    "---\n",
    "### Key Concepts:\n",
    "- **Graph Representation**: A graph is represented by an adjacency matrix **B**, where each entry **bᵢⱼ** indicates the presence or weight of an edge between nodes **i** and **j**. In undirected graphs, the matrix is symmetric.\n",
    "- **Graph Embedding**: Maps nodes into feature vectors, capturing relationships between nodes, typically through matrix factorization of **B ≈ UVᵀ**.\n",
    "- **Logistic Matrix Factorization**: Used for binary adjacency matrices, modeled as Bernoulli distributions with sigmoid-applied dot products of node embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b10e128",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Architecture for Graph Embedding\n",
    "---\n",
    "### Neural Network for Graph Embedding:\n",
    "- Input: One-hot encoded index for each node\n",
    "- Output: Binary vector representing whether each node is connected to the input node\n",
    "- Sigmoid activation used to predict connections between nodes\n",
    "- The architecture resembles Word2Vec's Skip-Gram with Negative Sampling (SGNS)\n",
    "\n",
    "   ![hingeloss](graphembed.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cedfe0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Negative Sampling in Graph Embeddings\n",
    "---\n",
    "### Negative Sampling:\n",
    "- Handles the imbalance between connected and unconnected nodes\n",
    "- Only a subset of negative (non-connected) nodes is sampled for training efficiency\n",
    "- Similar to how negative sampling is used in Word2Vec for handling non-context words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9831d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Graph Embeddings vs. Word Embeddings\n",
    "---\n",
    "### Comparison:\n",
    "- Both methods map items (nodes or words) into vector spaces that capture their relationships\n",
    "- **Graph Embeddings**: Nodes have distinct neighbors\n",
    "- **Word Embeddings**: Words appear multiple times in various contexts\n",
    "- The methods differ in data structure but share similar embedding techniques\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96d2d3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Handling Arbitrary Edge Counts\n",
    "---\n",
    "### Binary vs. Weighted Edges:\n",
    "- **Binary Edges**: **bᵢⱼ = 1** if there is an edge, **bᵢⱼ = 0** otherwise\n",
    "- **Weighted Edges**: Each edge **(i, j)** has an arbitrary count **cᵢⱼ**, representing connection strength or frequency\n",
    "\n",
    "### Neural Architecture Adaptation:\n",
    "- **Input**: One-hot encoded vector representing node **i**\n",
    "- **Output**: One-hot encoded vector for node **j**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20b650",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Negative Sampling and Training Process\n",
    "---\n",
    "### Negative Sampling:\n",
    "- **purpose**: handle non-existent edges\n",
    "- **Positive Samples**: Actual edges **(i, j)** \n",
    "- **Negative Samples**: Random node pairs without edges\n",
    "- **Sampling Rate**: **k** negative samples for each positive sample\n",
    "\n",
    "### Training:\n",
    "- **Loss Function**: Logistic loss with stochastic gradient descent (SGD) used to update embeddings **U** and **V**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d8600a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond One-Hop Structural Models\n",
    "---\n",
    "### Limitations of One-Hop Models:\n",
    "- Consider only immediate neighbors (one-hop) for embedding\n",
    "- Miss out on richer structural information\n",
    "\n",
    "### Enhancing Structural Information:\n",
    "- **Random Walks**: Traverse the graph to generate node sequences and capture indirect connections\n",
    "- **Affinity Measures**: Use methods like Adamic-Adar or Jaccard Coefficient to quantify connection strengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04838b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advanced Models for Graph Embeddings\n",
    "---\n",
    "### Advanced Graph Embedding Models:\n",
    "- **node2vec**: Extends random walks with parameters balancing breadth-first and depth-first search\n",
    "- **DeepWalk**: Uses random walks to generate node sequences and treats them like sentences for Word2Vec embedding learning\n",
    "- **Graph Neural Networks (GNNs)**: Learn embeddings by considering multi-hop relationships in an end-to-end fashion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d687dd09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multinomial Model for Graph Embeddings\n",
    "---\n",
    "### Multinomial vs Binary Models:\n",
    "- **Vanilla Skip-Gram**: Uses softmax to predict a multinomial distribution of context words\n",
    "- **SGNS**: Treats each context word as an independent binary prediction using sigmoid activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deec59a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training with Negative Sampling\n",
    "---\n",
    "### Training Process:\n",
    "- Each edge **(i, j)** is sampled based on its connection count **cᵢⱼ**\n",
    "- **Negative Samples**: Drawn for each positive pair based on frequency-adjusted distribution\n",
    "\n",
    "### Loss Function:\n",
    "- Combines positive and negative samples, minimizing the log-likelihood loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb85f76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison to Recommender Systems\n",
    "---\n",
    "### Graph Embeddings vs Recommender Systems:\n",
    "- **Similarity**: Both focus on positive interactions and sampled negatives\n",
    "- **Difference**: Graph embeddings handle node relationships, while recommender systems focus on user-item interactions\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
